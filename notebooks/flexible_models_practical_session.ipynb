{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8b744fdc",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "8b744fdc"
      },
      "source": [
        "\n",
        "The aim of the exercise is to get familiar with the histogram gradient-boosting in scikit-learn. Besides, we will use this model within a cross-validation framework in order to inspect internal parameters found via grid-search.\n",
        "\n",
        "We will use the California housing dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4378757e",
      "metadata": {
        "id": "4378757e"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "data, target = fetch_california_housing(return_X_y=True, as_frame=True)\n",
        "target *= 100  # Rescale target to k$\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dbca925",
      "metadata": {
        "id": "0dbca925"
      },
      "source": [
        "### üìù TODO: First, create a histogram gradient boosting regressor. You can set the trees number to be large, and configure the model to use early-stopping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "325dd56d",
      "metadata": {
        "lines_to_next_cell": 0,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "325dd56d",
        "outputId": "c976177b-d541-49f5-e45d-17bdb3158a5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Score: 0.9177551717531773\n",
            "Test Score: 0.8482980797850956\n"
          ]
        }
      ],
      "source": [
        "random_state=42\n",
        "\n",
        "# Define the Histogram Gradient Boosting Regressor\n",
        "hgb_regressor = HistGradientBoostingRegressor(\n",
        "    max_iter=1000,  # Large number of trees\n",
        "    early_stopping=True,  # Enable early stopping\n",
        "    scoring=\"neg_mean_squared_error\",\n",
        "    validation_fraction=0.1,\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "hgb_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Print model performance\n",
        "print(\"Training Score:\", hgb_regressor.score(X_train, y_train))\n",
        "print(\"Test Score:\", hgb_regressor.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "473e9a5f",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "473e9a5f"
      },
      "source": [
        "### üìù TODO: Use `RandomizedSearchCV` with `n_iter=20` to find the best set of\n",
        "hyperparameters by tuning the following parameters of the `model`:\n",
        "- max_depth: [3, 8];\n",
        "- max_leaf_nodes: [15, 31];\n",
        "- learning_rate: [0.1, 1].\n",
        "\n",
        "Notice that in the notebook \"Hyperparameter tuning by randomized-search\" we\n",
        "pass distributions to be sampled by the `RandomizedSearchCV`. In this case we\n",
        "define a fixed grid of hyperparameters to be explored. Using a `GridSearchCV`\n",
        "instead would explore all the possible combinations on the grid, which can be\n",
        "costly to compute for large grids, whereas the parameter `n_iter` of the\n",
        "`RandomizedSearchCV` controls the number of different random combination that\n",
        "are evaluated. Notice that setting `n_iter` larger than the number of possible\n",
        "combinations in a grid would lead to repeating\n",
        "already-explored combinations. Here, this can't happen since the learning rate is sampled from a uniform so the number of possible combinations is infinite."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "96d4dc06",
      "metadata": {
        "lines_to_next_cell": 0,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96d4dc06",
        "outputId": "d837a199-7d60-4263-a2e4-b1e00d69181f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 8 is smaller than n_iter=20. Running 8 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'max_leaf_nodes': 31, 'max_depth': 8, 'learning_rate': 0.1}\n",
            "Best Score: -2159.1499843772317\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import uniform, randint\n",
        "import numpy as np\n",
        "\n",
        "# Define parameter grid\n",
        "param_distributions = {\n",
        "    \"max_depth\": [3, 8],\n",
        "    \"max_leaf_nodes\": [15, 31],\n",
        "    \"learning_rate\": [0.1, 1]\n",
        "}\n",
        "\n",
        "# Initialize the model\n",
        "hgb_regressor = HistGradientBoostingRegressor(\n",
        "    max_iter=1000,  # Keep a large number of trees\n",
        "    early_stopping=True,\n",
        "    scoring=\"neg_mean_squared_error\",\n",
        "    validation_fraction=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Perform RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=hgb_regressor,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=20,\n",
        "    scoring=\"neg_mean_squared_error\",\n",
        "    cv=5,  # 5-fold cross-validation\n",
        "    random_state=42,\n",
        "    n_jobs=-1  # Use all available CPUs\n",
        ")\n",
        "\n",
        "# Fit RandomizedSearchCV\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Best hyperparameters\n",
        "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
        "print(\"Best Score:\", random_search.best_score_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "829d12ca",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "829d12ca"
      },
      "source": [
        "üìù TODO: Secondly, we will run our experiment through cross-validation. In this regard, define a 5-fold cross-validation. Besides, be sure to shuffle the data. Subsequently, use the function sklearn.model_selection.cross_validate to run the cross-validation. You should also set return_estimator=True, so that we can investigate the inner model trained via cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04903309",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "04903309"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_validate, KFold\n",
        "# CODE HERE\n",
        "cv =\n",
        "cross_val_results ="
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbe3442d",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "cbe3442d"
      },
      "source": [
        "### üìù TODO: Now that we got the cross-validation results, print out the mean and standard deviation score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aad3c966",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "aad3c966"
      },
      "outputs": [],
      "source": [
        "cv_mean, cv_sd ="
      ]
    },
    {
      "cell_type": "markdown",
      "id": "102110b3",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "102110b3"
      },
      "source": [
        "### üìù TODO: Then inspect the estimator entry of the results and check the best parameters values. Besides, check the number of trees used by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa72401a",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "aa72401a"
      },
      "outputs": [],
      "source": [
        "# CODE HERE\n",
        "for estimator in :"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d990891",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "3d990891"
      },
      "source": [
        "###¬†üìù TODO: Inspect the results of the inner CV for each estimator of the outer CV. Aggregate the mean test score for each parameter combination and make a box plot of these scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee14cc8d",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "ee14cc8d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# CODE HERE\n",
        "inner_cv_results = []\n",
        "for cv_ix, estimator_ in :\n",
        "    results =\n",
        "\n",
        "inner_cv_results = pd.concat(inner_cv_results, axis=1).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a172b5c",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "8a172b5c"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sort the inner_cv_results by the mean of the columns\n",
        "sorted_inner_cv_results = inner_cv_results.mean().sort_values().index\n",
        "inner_cv_results = inner_cv_results[sorted_inner_cv_results]\n",
        "\n",
        "color = {\"whiskers\": \"black\", \"medians\": \"black\", \"caps\": \"black\"}\n",
        "inner_cv_results.plot.box(vert=False, color=color)\n",
        "plt.xlabel(\"R2 score\")\n",
        "plt.ylabel(\"Parameters\")\n",
        "_ = plt.title(\n",
        "    \"Inner CV results with parameters\\n\" \"(max_depth, max_leaf_nodes, learning_rate)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5e1ce0a",
      "metadata": {
        "id": "f5e1ce0a"
      },
      "source": [
        "We see that the first 4 ranked set of parameters are very close. We could select any of these 4 combinations. It coincides with the results we observe when inspecting the best parameters of the outer CV."
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "main_language": "python"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}